{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 20,10\n",
    "import numpy as np\n",
    "import glob\n",
    "from scipy import stats\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/racheldilley/Documents/Metis/git_repos/mta-project-1/CSVs\n"
     ]
    }
   ],
   "source": [
    "cd CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extension = 'txt'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "\n",
    "#combine all files in the list\n",
    "data = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove spaces from column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.columns.str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add Date_Time column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date_Time'] = pd.to_datetime(data['DATE'], cache=True) + pd.to_timedelta(data['TIME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Turnstile (Combining SCP,C/A, Station and Unit) and Weekday column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Turnstile'] = data['C/A'] + data['UNIT'] + data['SCP'] + data['STATION']\n",
    "data['DATE'] = pd.to_datetime(data['DATE'], cache=True)\n",
    "data['Week_Day'] = data['DATE'].dt.day_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find total traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add shifted columns to show previous entries and exits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All turnstiles have cumulative counts, previous data needs to be subtraced from current data \n",
    "#to find total entries/exits\n",
    "grouped_time=data\n",
    "grouped_time[\"PREV_TIME\"] = data.groupby([\"Turnstile\"]).TIME.shift(1)\n",
    "grouped_time[\"PREV_EXITS\"] = data.groupby([\"Turnstile\"]).EXITS.shift(1)\n",
    "grouped_time[\"PREV_ENTRIES\"] = data.groupby([\"Turnstile\"]).ENTRIES.shift(1)\n",
    "#grouped_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop first row of each turnstile b/c of shift down\n",
    "grouped_time.dropna(subset=[\"PREV_ENTRIES\"], axis=0, inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove counter errors from total entries/exits that occur from counters resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_counts_exits(row, max_counter):\n",
    "    counter_ex = row[\"EXITS\"] - row[\"PREV_EXITS\"]\n",
    "    if counter_ex < 0:\n",
    "        # Maybe counter is reversed?\n",
    "        counter_ex = -counter_ex\n",
    "    if counter_ex > max_counter:\n",
    "        # Maybe counter was reset to 0? \n",
    "        #print(row[\"EXITS\"], row[\"PREV_EXITS\"])\n",
    "        counter = min(row[\"EXITS\"], row[\"PREV_EXITS\"])\n",
    "    if counter_ex > max_counter:\n",
    "        # Check it again to make sure we're not still giving a counter that's too big\n",
    "        return 0\n",
    "    return counter_ex\n",
    "\n",
    "def get_daily_counts_entries(row, max_counter):\n",
    "    counter_ent = row[\"ENTRIES\"] - row[\"PREV_ENTRIES\"]\n",
    "    if counter_ent < 0:\n",
    "        # Maybe counter is reversed?\n",
    "        counter_ent = -counter_ent\n",
    "    if counter_ent > max_counter:\n",
    "        # Maybe counter was reset to 0? \n",
    "        #print(row[\"ENTRIES\"], row[\"PREV_ENTRIES\"])\n",
    "        counter = min(row[\"ENTRIES\"], row[\"PREV_ENTRIES\"])\n",
    "    if counter_ent > max_counter:\n",
    "        # Check it again to make sure we're not still giving a counter that's too big\n",
    "        return 0\n",
    "    return counter_ent\n",
    "\n",
    "\n",
    "# If counter is > 1Million, then the counter might have been reset.  \n",
    "# Just set it to zero as different counters have different cycle limits\n",
    "# It'd probably be a good idea to use a number even significantly smaller than 1 million as the limit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_time[\"TOTAL_ENTRIES\"] = grouped_time.apply(get_daily_counts_entries, axis=1, max_counter=1000000)\n",
    "grouped_time[\"TOTAL_EXITS\"] = grouped_time.apply(get_daily_counts_exits, axis=1, max_counter=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find total traffic\n",
    "grouped_time[\"TOTAL_TRAFFIC\"] = grouped_time[\"TOTAL_EXITS\"] + grouped_time[\"TOTAL_ENTRIES\"]\n",
    "grouped_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove total traffic outliers that are 3 sigmas from mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_time = grouped_time[(np.abs(stats.zscore(grouped_time['TOTAL_TRAFFIC'])) < 3)] #filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop uneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_time.drop(['C/A','UNIT','SCP','LINENAME','DESC', 'ENTRIES', 'EXITS', 'PREV_TIME', 'PREV_ENTRIES', 'PREV_EXITS', 'TOTAL_ENTRIES', 'TOTAL_EXITS'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped_day = data.groupby(['DATE','Turnstile','STATION'], as_index=False).agg({'ENTRIES': ['min', 'max'], 'EXITS': ['min', 'max']})\n",
    "#grouped = data.groupby(['STATION','DATE']).agg({'ENTRIES': ['min', 'max'], 'EXITS': ['min', 'max']})\n",
    "#grouped = grouped.set_index('STATION')\n",
    "#grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped.columns = [ 'Station', 'Date', 'ENTRIES_MIN', 'ENTRIES_MAX', 'EXITS_MIN', 'EXITS_MAX']\n",
    "#grouped_day.columns = [ 'Date', 'Turnstile', 'Station','ENTRIES_MIN', 'ENTRIES_MAX', 'EXITS_MIN', 'EXITS_MAX']\n",
    "#grouped.columns = ['ENTRIES_MIN', 'ENTRIES_MAX', 'EXITS_MIN', 'EXITS_MAX']\n",
    "#grouped.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find total entries exits, and traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped_day['Total_Entries'] = grouped_day['ENTRIES_MAX'] - grouped_day['ENTRIES_MIN']\n",
    "#grouped_day['Total_Exits'] = grouped_day['EXITS_MAX'] - grouped_day['EXITS_MIN']\n",
    "#grouped_day['Total_Traffic'] = grouped_day['Total_Exits'] + grouped_day['Total_Entries']\n",
    "#grouped_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped_day.drop(['ENTRIES_MIN','ENTRIES_MAX','EXITS_MIN','EXITS_MAX', 'Total_Entries', 'Total_Exits', ], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Outlier data from Total_traffic column (Removing anything outside 3 sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped_day = grouped_day[(np.abs(stats.zscore(grouped_day['Total_Traffic'])) < 3)] #filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding top 10 visited stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_ridership = grouped_day.groupby(['Station']).Total_Traffic.sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_ridership.sort_values(by=['Total_Traffic'],inplace = True, ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Row_list =[] \n",
    "indices = []\n",
    "  \n",
    "# Iterate over each row \n",
    "for index, rows in Total_ridership[:10].iterrows(): \n",
    "    # Create list for the current row \n",
    "#    my_list =[rows.STATION, rows.Total_Traffic] \n",
    "    my_list =rows.Total_Traffic \n",
    "    station_list = rows.Station\n",
    "    # append the list to the final list \n",
    "    Row_list.append(my_list) \n",
    "    indices.append(station_list)\n",
    "Total_ridership.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylim(10000000,21000000)\n",
    "plt.xlabel(\"Station\")\n",
    "plt.ylabel(\"Total Traffic\")\n",
    "plt.title(\"Total traffic for the period for top 10 busiest stations\")\n",
    "plt.bar(indices, Row_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 Busy Stations, We will focus on top5 of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find high traffic days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_Weekly_ridership = grouped_day.groupby(['Week_Day']).Total_Traffic.sum().reset_index()\n",
    "#Total_Weekly_ridership = grouped_day.groupby(['Week_Day']).Total_Traffic.mean().reset_index()\n",
    "Total_Weekly_ridership.sort_values(by=['Total_Traffic'],inplace = True, ascending = False)\n",
    "Total_Weekly_ridership.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.ylim(1610000000000,1620000000000)\n",
    "plt.ylim(40000000,140000000)\n",
    "plt.bar(Total_Weekly_ridership['Week_Day'], Total_Weekly_ridership['Total_Traffic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this it seems like weekends see much less traffic compared to weekdays. So WWTF should focus more on weekdays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top5 = Total_ridership['Station'][0:5].tolist()\n",
    "Top5_stations = grouped_day[grouped_day['Station'].isin(Top5)]\n",
    "Top5_station_weekly = Top5_stations.groupby(['Station','Week_Day']).Total_Traffic.sum().reset_index()\n",
    "Top5_station_Daily = Top5_stations.groupby(['Station','Date']).Total_Traffic.sum().reset_index()\n",
    "#Top5_station_weekly = Top5_stations.groupby(['Station','Week_Day']).Total_Traffic.mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weekly Plot for Top5 stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for stations in Top5:\n",
    "    station = Top5_station_weekly[Top5_station_weekly['Station'] == stations]\n",
    "#    print(station)\n",
    "    plt.plot(station['Week_Day'],station['Total_Traffic'],label = stations)\n",
    "#plt.ylim(32500000000,40700000000)\n",
    "#plt.ylim(400000000,405000000)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot seems like top5 stations are also showing a dip on weekends, but PATH NEW WTC station is oblitrating seeing the trend properly. This needs to be investigated further. Also, to see the trend for rest of the station we will do another plot removing the data for PATH NEW WTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stations in Top5:\n",
    "    if stations != \"PATH NEW WTC\" :\n",
    "        station = Top5_station_weekly[Top5_station_weekly['Station'] == stations]\n",
    "#    print(station)\n",
    "        plt.plot(station['Week_Day'],station['Total_Traffic'],label = stations)\n",
    "#plt.ylim(32500000000,40700000000)\n",
    "#plt.ylim(400000000,405000000)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also showing the dip in weekend more clearly. But GRD CNTRL-42 ST shows peak on thursday - which needs to be investigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stations in Top5:\n",
    "    station = Top5_station_Daily[Top5_station_Daily['Station'] == stations]\n",
    "#    print(station)\n",
    "    plt.plot(station['Date'],station['Total_Traffic'],label = stations)\n",
    "#plt.ylim(32500000000,40700000000)\n",
    "#plt.ylim(400000000,405000000)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is also showing some abnormal values for some of the days for PATH NEW WTC. Two of these dates are looking on thursdays. We will look at rest of the stations removing PATH NEW WTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stations in Top5:\n",
    "    if stations != \"PATH NEW WTC\" :\n",
    "        station = Top5_station_Daily[Top5_station_Daily['Station'] == stations]\n",
    "#    print(station)\n",
    "        plt.plot(station['Date'],station['Total_Traffic'],label = stations)\n",
    "#plt.ylim(32500000000,40700000000)\n",
    "#plt.ylim(400000000,405000000)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot also seems to have some outliers. But In general trend is clear Traffic takes a dip on weekends and remains kind of static on weekdays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find peak hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find high traffic areas for games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Boroughs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read location data to get borough info and latitude/longitude information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df = pd.read_csv('http://web.mta.info/developers/data/nyct/subway/Stations.csv')\n",
    "#print(loc_data.Borough.unique())\n",
    "loc_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add borough_name column\n",
    "burough_dict = { 'Q': 'Queens', 'M': 'Manhattan', 'Bk':'Brooklyn', 'Bx': 'Bronx', 'SI': 'Staten Island'}\n",
    "loc_df['Borough_Name'] = loc_df['Borough'].map(burough_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop uneeded columns\n",
    "loc_df.drop(['Borough','Line','Station ID','Complex ID', 'GTFS Stop ID', 'Line', 'Structure', \n",
    "               'Daytime Routes', 'North Direction Label', 'South Direction Label', 'ADA', 'ADA Notes'], axis=1, inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename columns in loc_data to merge on STATION and DIVISION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stations were found to have the same but located in diff areas so DIVISION was also used\n",
    "loc_df.rename(columns = {'Stop Name':'STATION', 'Division':'DIVISION','GTFS Latitude':'Latitude','GTFS Longitude':'Longitude'}, inplace = True) \n",
    "loc_df['STATION'] = loc_df['STATION'].str.upper()\n",
    "#join with grouped_time\n",
    "station_loc_df = grouped_time.merge(loc_df, how = 'inner', on = ['STATION'])\n",
    "station_loc_df.tail()\n",
    "print(station_loc_df[station_loc_df['STATION']=='28 ST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph number of stations in each borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_station_count_df = station_loc_df.groupby('Borough_Name')['STATION'].nunique().reset_index()\n",
    "#borough_station_count_df\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 4)\n",
    "plt.xlabel(\"Borough\")\n",
    "plt.ylabel(\"Number of Stations\")\n",
    "plt.title(\"Number of Stations in Each Borough\")\n",
    "plt.bar(borough_station_count_df['Borough_Name'], borough_station_count_df['STATION'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph total traffic in each Borough "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_traffic_df = station_loc_df.groupby('Borough_Name').TOTAL_TRAFFIC.sum().reset_index()\n",
    "plt.xlabel(\"Borough\")\n",
    "plt.ylabel(\"Total Traffic\")\n",
    "plt.title(\"Total Traffic in Each Borough\")\n",
    "plt.bar(borough_traffic_df['Borough_Name'], borough_traffic_df['TOTAL_TRAFFIC'])\n",
    "#borough_traffic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find top stations in each borough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find top 5 stations in each borough and graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_top5_df = station_loc_df.groupby(['Borough_Name','STATION']).agg({'TOTAL_TRAFFIC':'sum'})\n",
    "borough_top5_df = borough_top5_df['TOTAL_TRAFFIC'].groupby('Borough_Name', group_keys=False)\n",
    "borough_top5_df = borough_top5_df.apply(lambda x: x.sort_values(ascending=False).head(5))\n",
    "borough_top5_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 20,10\n",
    "plt.title(\"Top 5 Busiest Stations in Each Borough\")\n",
    "plt.ylabel(\"Total Traffic\")\n",
    "borough_top5_df.unstack().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find stations in the highest income areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import income data\n",
    "income_data = pd.read_csv('https://raw.githubusercontent.com/hollyyuqizheng/transportation-transformation/master/data/new_york/new%20york%20income%20data.csv')\n",
    "#print(loc_data.Borough.unique())\n",
    "income_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new df grouped by latitude, longitude, and station\n",
    "station_lat_long_df = station_loc_df.groupby(['STATION','Latitude','Longitude','Borough_Name'], as_index=False ).TOTAL_TRAFFIC.sum()\n",
    "#station_lat_long_df.drop(['TOTAL_TRAFFIC'], axis=1, inplace =True)\n",
    "#station_lat_long_df.info()\n",
    "station_lat_long_df.duplicated(subset=['STATION']) \n",
    "print(station_lat_long_df[station_lat_long_df['STATION']==True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find average income for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#row from station_income_df as argument\n",
    "#loop through income_df to find closest zip code based on latitude and longitude\n",
    "#return Avg income of closest zip code to station\n",
    "def find_min_dist(row):\n",
    "    min_dist = 1000\n",
    "    idx = 0\n",
    "    for index, r in income_data.iterrows():\n",
    "        l = r[1].split(\", \")\n",
    "        l = [float(i) for i in l]\n",
    "        a = ((l[0] - row['Latitude'])**2 + (l[1] - row['Longitude'])**2)\n",
    "        a = abs(a)\n",
    "        dist = math.sqrt(a)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            idx = index\n",
    "    return income_data.iloc[idx]['Avg. Income/H/hold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_income_df = station_lat_long_df.copy()\n",
    "station_income_df['Avg_Income'] = station_income_df.apply(find_min_dist, axis=1)\n",
    "station_income_df.sort_values('Avg_Income', ascending=False, inplace=True)\n",
    "station_income_df.head(10)\n",
    "station_income_df.duplicated(subset=['STATION']) \n",
    "print(station_income_df[station_income_df['STATION']==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_income_df = station_income_df[['Borough_Name','STATION','Avg_Income', 'TOTAL_TRAFFIC']].head(10)\n",
    "station_income_df.duplicated(subset=['STATION']) \n",
    "print(station_income_df[station_income_df['STATION']==True])\n",
    "print(station_income_df[station_income_df['STATION']=='28 ST'])\n",
    "plt.xlabel(\"Stations\")\n",
    "plt.ylabel(\"Average Income\")\n",
    "plt.title(\"Top 10 stations with highest incomes in their area\")\n",
    "plt.bar(top_station_income_df['STATION'], top_station_income_df['Avg_Income'])\n",
    "\n",
    "top_traffic_stations_df = station_income_df.copy()\n",
    "top_traffic_stations_df = top_traffic_stations_df[['Borough_Name','STATION','Avg_Income', 'TOTAL_TRAFFIC']]\n",
    "top_traffic_stations_df.sort_values('TOTAL_TRAFFIC', ascending=False, inplace=True)\n",
    "top_traffic_stations_df = top_traffic_stations_df.head(10)\n",
    "print(station_income_df )\n",
    "print(top_traffic_stations_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find max and min of lat and long\n",
    "BBox = (station_lat_long_df.Longitude.min(),   station_lat_long_df.Longitude.max(),      \n",
    "         station_lat_long_df.Latitude.min(), station_lat_long_df.Latitude.max())\n",
    "BBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruh_m = plt.imread('NYC_Map.png') #load NYC Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (11,10))\n",
    "ax.scatter(station_lat_long_df.Longitude, station_lat_long_df.Latitude, zorder=1, alpha= 0.2, c='b', s=13)\n",
    "ax.set_title('Plotting Stations in NYC')\n",
    "ax.set_xlim(BBox[0],BBox[1])\n",
    "ax.set_ylim(BBox[2],BBox[3])\n",
    "ax.imshow(ruh_m, zorder=0, extent = BBox, aspect= 'equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
